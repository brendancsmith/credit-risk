{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# suppress debugging warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from src import preprocessing\n",
    "\n",
    "if \"df_backup\" not in globals():\n",
    "    # takes 2-4min\n",
    "    df = preprocessing.load_data()\n",
    "    df_backup = df.copy()\n",
    "else:\n",
    "    df = df_backup.copy()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first few rows\n",
    "print(\"First few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "display(df.isnull().sum().where(lambda x: x > 0).dropna())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "display(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that are independent of the target variable\n",
    "INDEP_COLS = [\n",
    "    'emp_title', \n",
    "    'id', \n",
    "    'member_id', \n",
    "    'policy_code', \n",
    "    'title', \n",
    "    'url',\n",
    "]\n",
    "\n",
    "# Remove columns that could cause data leakage\n",
    "LEAKAGE_COLS = [\n",
    "    'collection_recovery_fee', \n",
    "    'debt_settlement_flag_date',\n",
    "    'debt_settlement_flag', \n",
    "    'deferral_term',\n",
    "    'disbursement_method', \n",
    "    'funded_amnt_inv',\n",
    "    'funded_amnt', \n",
    "    'hardship_amount',\n",
    "    'hardship_dpd',\n",
    "    'hardship_end_date',\n",
    "    'hardship_flag',\n",
    "    'hardship_flag',\n",
    "    'hardship_length',\n",
    "    'hardship_length',\n",
    "    'hardship_loan_status',\n",
    "    'hardship_reason',\n",
    "    'hardship_start_date',\n",
    "    'hardship_status',\n",
    "    'hardship_type',\n",
    "    'initial_list_status',\n",
    "    'issue_d',\n",
    "    'last_credit_pull_d', \n",
    "    'last_fico_range_high',\n",
    "    'last_fico_range_low', \n",
    "    'last_pymnt_amnt',\n",
    "    'last_pymnt_d', \n",
    "    'next_pymnt_d',\n",
    "    'out_prncp_inv', \n",
    "    'out_prncp',\n",
    "    'payment_plan_start_date',\n",
    "    'pymnt_plan', \n",
    "    'recoveries', \n",
    "    'settlement_amount',\n",
    "    'settlement_date',\n",
    "    'settlement_percentage',\n",
    "    'settlement_status',\n",
    "    'settlement_term',\n",
    "    'total_pymnt_inv', \n",
    "    'total_pymnt', \n",
    "    'total_rec_int',\n",
    "    'total_rec_late_fee', \n",
    "    'total_rec_prncp', \n",
    "]\n",
    "\n",
    "cols_to_drop = INDEP_COLS + LEAKAGE_COLS\n",
    "df = preprocessing.drop_cols(df, cols=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features with too many missing values\n",
    "df = preprocessing.drop_sparse_cols(df, missing_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to unix timestamps\n",
    "date_cols = ['earliest_cr_line']\n",
    "df = preprocessing.convert_dates(df, cols=date_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle emp_length\n",
    "from src.preprocessing import extract_digits\n",
    "\n",
    "df[df['emp_length'] == '< 1 year'] = 0\n",
    "df = extract_digits(df, cols=['emp_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract digits from string values\n",
    "df = preprocessing.extract_digits(df, cols=['term', 'zip_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for ordinal categorical variables\n",
    "from src.feature_engineering import index_categories\n",
    "\n",
    "grades = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "df = index_categories(df, 'grade', categories=grades)\n",
    "\n",
    "sub_grades = [f'{g}{s}' for g in grades for s in range(1, 6)]\n",
    "df = index_categories(df, 'sub_grade', categories=sub_grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "df = preprocessing.impute_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_engineering import index_categories, frequency_encoding, onehot_encoding\n",
    "\n",
    "# Encode target variable\n",
    "df = index_categories(df, 'loan_status', categories=['Fully Paid', 'Charged Off'], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode low-cardinality categorical variables using one-hot encoding\n",
    "ONEHOT_CATS = [\n",
    "    'application_type',\n",
    "    'home_ownership',\n",
    "    'purpose',\n",
    "    'verification_status'\n",
    "]\n",
    "\n",
    "df = onehot_encoding(df, cols=ONEHOT_CATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode high-cardinality categorical variables using frequency encoding\n",
    "FREQ_CATS = [\n",
    "    'addr_state'\n",
    "]\n",
    "\n",
    "df = frequency_encoding(df, cols=FREQ_CATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop highly correlated feature pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import algebra, visualize\n",
    "from src.feature_engineering import drop_corr_pairs\n",
    "\n",
    "cols = df.columns.values\n",
    "corr = algebra.correlation_matrix(df.drop(columns=['loan_status']))\n",
    "\n",
    "# plot correlation matrix\n",
    "visualize.correlation_matrix(corr)\n",
    "\n",
    "df = drop_corr_pairs(df, corr)\n",
    "\n",
    "removed = np.setdiff1d(cols, df.columns.values)\n",
    "print(f\"Removed columns:\\n\\t{\"\\n\\t\".join(sorted(removed))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import scale_features\n",
    "\n",
    "df = scale_features(df, target='loan_status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take another peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.class_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import training\n",
    "\n",
    "X_resampled, y_resampled = training.resample(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import create_model\n",
    "\n",
    "# Takes ~20min on 16\" M2 MBP\n",
    "if \"rfe\" not in globals():\n",
    "    from sklearn.feature_selection import RFECV\n",
    "\n",
    "    rfe_model = create_model(eval_metric='logloss')\n",
    "\n",
    "    # Create a model to determine the number of features for feature selection\n",
    "    rfe = RFECV(\n",
    "        estimator=rfe_model, cv=FOLDS, scoring='roc_auc', n_jobs=(-1))\n",
    "    rfe.fit(X_train, y_train)\n",
    "    n_features_to_select = rfe.n_features_\n",
    "\n",
    "# Plot the number of features vs. cross-validation scores\n",
    "visualize.plot_rfe(rfe)\n",
    "\n",
    "rfe_score = rfe.score(X_test, y_test)\n",
    "print(f\"Optimal number of features to select: {rfe.n_features_}\") # type: ignore\n",
    "print(f\"RFE Test Score: {rfe_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns[rfe.support_]\n",
    "removed = X_train.columns[~rfe.support_]\n",
    "X_train = pd.DataFrame(rfe.transform(X_train), columns=cols) # type: ignore\n",
    "X_test = pd.DataFrame(rfe.transform(X_test), columns=cols) # type: ignore\n",
    "\n",
    "print(f\"Removed columns:\\n\\t{\"\\n\\t\".join(sorted(removed))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlations between the features in X_train and the target variable y_train\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "correlations = (df_train.corr()['loan_status']\n",
    "    .sort_values(ascending=False)\n",
    "    .to_frame()\n",
    "    .drop('loan_status')\n",
    ")\n",
    "\n",
    "visualize.correlations(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump((X_train, X_test, y_train, y_test), '../data/processed/accepted_2007_to_2018Q4.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
