{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis & Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from src import preprocessing\n",
    "\n",
    "if \"df_backup\" not in globals():\n",
    "    # takes 2-4min\n",
    "    df = preprocessing.load_data()\n",
    "    df_backup = df.copy()\n",
    "else:\n",
    "    df = df_backup.copy()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first few rows\n",
    "print(\"First few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "display(df.isnull().sum())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "display(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that are independent of the target variable\n",
    "INDEP_COLS = [\n",
    "    'emp_title', \n",
    "    'id', \n",
    "    'member_id', \n",
    "    'policy_code', \n",
    "    'title', \n",
    "    'url',\n",
    "]\n",
    "\n",
    "# Remove columns that could cause data leakage\n",
    "LEAKAGE_COLS = [\n",
    "    'collection_recovery_fee', \n",
    "    'debt_settlement_flag', \n",
    "    'disbursement_method', \n",
    "    'funded_amnt_inv',\n",
    "    'funded_amnt', \n",
    "    'hardship_flag',\n",
    "    'initial_list_status',\n",
    "    'issue_d',\n",
    "    'last_credit_pull_d', \n",
    "    'last_fico_range_high',\n",
    "    'last_fico_range_low', \n",
    "    'last_pymnt_amnt',\n",
    "    'last_pymnt_d', \n",
    "    'next_pymnt_d',\n",
    "    'out_prncp_inv', \n",
    "    'out_prncp',\n",
    "    'pymnt_plan', \n",
    "    'recoveries', \n",
    "    'total_pymnt_inv', \n",
    "    'total_pymnt', \n",
    "    'total_rec_int',\n",
    "    'total_rec_late_fee', \n",
    "    'total_rec_prncp', \n",
    "]\n",
    "\n",
    "cols_to_drop = INDEP_COLS + LEAKAGE_COLS\n",
    "df = preprocessing.drop_cols(df, cols=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features with too many missing values\n",
    "df = preprocessing.drop_sparse_cols(df, missing_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to unix timestamps\n",
    "date_cols = ['earliest_cr_line']\n",
    "df = preprocessing.convert_dates(df, cols=date_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "df = preprocessing.impute_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_engineering import index_categories, frequency_encoding, onehot_encoding\n",
    "\n",
    "# Encode target variable\n",
    "df = index_categories(df, 'loan_status', categories=['Charged Off', 'Fully Paid'], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for ordinal categorical variables\n",
    "grades = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "df = index_categories(df, 'grade', categories=grades)\n",
    "\n",
    "sub_grades = [f'{g}{s}' for g in grades for s in range(1, 6)]\n",
    "df = index_categories(df, 'sub_grade', categories=sub_grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode low-cardinality categorical variables using one-hot encoding\n",
    "ONEHOT_CATS = [\n",
    "    'application_type',\n",
    "    'home_ownership',\n",
    "    'purpose',\n",
    "    'term',\n",
    "    'verification_status'\n",
    "]\n",
    "\n",
    "df = onehot_encoding(df, cols=ONEHOT_CATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode high-cardinality categorical variables using frequency encoding\n",
    "FREQ_CATS = [\n",
    "    'addr_state'\n",
    "]\n",
    "\n",
    "df = frequency_encoding(df, cols=FREQ_CATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop highly correlated feature pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import algebra, visualize\n",
    "from src.feature_engineering import drop_corr_pairs\n",
    "\n",
    "cols = df.columns.values\n",
    "corr = algebra.correlation_matrix(df)\n",
    "\n",
    "# plot correlation matrix\n",
    "visualize.correlation_matrix(corr)\n",
    "\n",
    "df = drop_corr_pairs(df, corr)\n",
    "\n",
    "removed = np.setdiff1d(cols, df.columns.values)\n",
    "print(f\"Removed columns:\\n\\t{\"\\n\\t\".join(removed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_engineering import extract_integers, frequency_encoding, new_features\n",
    "\n",
    "df = extract_integers(df, cols=['emp_length', 'zip_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import scale_features\n",
    "\n",
    "df = scale_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take another peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.class_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import training\n",
    "\n",
    "X_resampled, y_resampled = training.resample(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import create_model\n",
    "\n",
    "rfe_model = create_model(eval_metric='logloss')\n",
    "\n",
    "if \"n_features_to_select\" in globals():\n",
    "    from sklearn.feature_selection import RFE\n",
    "\n",
    "    # Create a model to determine the most important features\n",
    "    rfe = RFE(\n",
    "        estimator=rfe_model,\n",
    "        n_features_to_select=n_features_to_select # type: ignore\n",
    "    )\n",
    "    rfe.fit(X_train, y_train)\n",
    "else:\n",
    "    from sklearn.feature_selection import RFECV\n",
    "\n",
    "    # Create a model to determine the number of features for feature selection\n",
    "    rfe = RFECV(\n",
    "        estimator=rfe_model, cv=FOLDS, scoring='roc_auc', n_jobs=(-1))\n",
    "    rfe.fit(X_train, y_train)\n",
    "    n_features_to_select = rfe.n_features_\n",
    "\n",
    "    # Plot rfe.cv_results_\n",
    "    plt.figure()\n",
    "    plt.title(\"RFE with cross-validation\")\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score\")\n",
    "    plt.plot(rfe.cv_results_[\"n_features\"], rfe.cv_results_[\"mean_test_score\"])\n",
    "    cv_score = rfe.cv_results_[\"mean_test_score\"][n_features_to_select - 1]\n",
    "    plt.scatter(n_features_to_select, cv_score, marker=\"|\", s=100)\n",
    "    plt.show()\n",
    "\n",
    "score = rfe.score(X_test, y_test)\n",
    "print(f\"Optimal number of features to select: {n_features_to_select}\") # type: ignore\n",
    "print(f\"RFE Test Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns[rfe.support_]\n",
    "X_train = pd.DataFrame(rfe.transform(X_train), columns=cols) # type: ignore\n",
    "X_test = pd.DataFrame(rfe.transform(X_test), columns=cols) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(eval_metric='logloss')\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "visualize.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "visualize.roc_curve(y_test, y_proba, roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f\"Cross-Validation ROC-AUC Scores: {cv_scores}\")\n",
    "print(f\"Mean Cross-Validation ROC-AUC Score: {cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlations between the features in X_train and the target variable y_train\n",
    "df_train = pd.concat([X_train, y_train])\n",
    "correlations = df_train.corr()['loan_status'].sort_values().to_frame()\n",
    "\n",
    "# plot the correlations\n",
    "visualize.correlations(correlations.drop('loan_status'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate permutation importance\n",
    "result = permutation_importance(\n",
    "    model, X_train, y_train, n_repeats=FOLDS, random_state=42)\n",
    "\n",
    "# Get importance values\n",
    "importances = result['importances_mean']\n",
    "\n",
    "# Create a DataFrame with feature names and importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "feature_importances = feature_importances.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Display the feature importances DataFrame\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(feature_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "### HP search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    \"max_depth\": stats.randint(2, 3),\n",
    "    \"learning_rate\": stats.uniform(loc=0.93, scale=0.07),\n",
    "    \"n_estimators\": stats.randint(100, 1000),\n",
    "    \"subsample\": stats.norm(0.85, scale=0.05),\n",
    "    \"colsample_bytree\": stats.uniform(loc=0.98, scale=0.02),\n",
    "}\n",
    "\n",
    "# Setup the randomized search with 50 iterations\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=create_model(eval_metric='logloss'),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30, # ~9sec/iter\n",
    "    cv=FOLDS,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    n_jobs=(-1)\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Best Model\n",
    "print(\"Best Parameters:\", search.best_params_)\n",
    "print(f\"Best ROC-AUC Score: {search.best_score_:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain with these hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = search.best_params_\n",
    "model_best = create_model(**best_params)\n",
    "model_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred_best = model_best.predict(X_test)\n",
    "y_proba_best = model_best.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Accuracy\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Optimized Accuracy: {accuracy_best:.4f}\")\n",
    "\n",
    "print(\"Optimized Classification Report:\\n\", classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_proba_best)\n",
    "print(f\"Optimized ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "visualize.roc_curve(y_test, y_proba_best, roc_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
