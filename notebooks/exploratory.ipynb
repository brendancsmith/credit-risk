{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis & Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from src import preprocessing\n",
    "\n",
    "if \"df_backup\" not in globals():\n",
    "    # takes 2-4min\n",
    "    df = preprocessing.load_data()\n",
    "    df_backup = df.copy()\n",
    "else:\n",
    "    df = df_backup.copy()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first few rows\n",
    "print(\"First few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "display(df.isnull().sum())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "display(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that are independent of the target variable\n",
    "# and those that could cause data leakage\n",
    "df = preprocessing.drop_cols(df)\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Remove features with too many missing values\n",
    "df = preprocessing.drop_sparse_cols(df)\n",
    "\n",
    "# Convert date columns to unix timestamps\n",
    "df = preprocessing.convert_dates(df)\n",
    "\n",
    "# Impute missing values\n",
    "df = preprocessing.impute_missing_values(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_engineering import encode_target, frequency_encoding, onehot_encoding\n",
    "\n",
    "# Encode target variable and categorical features\n",
    "df = encode_target(df)\n",
    "df = onehot_encoding(df)\n",
    "df = frequency_encoding(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop highly correlated feature pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import algebra, visualize\n",
    "from src.feature_engineering import drop_high_corr\n",
    "\n",
    "corr = algebra.correlation_matrix(df)\n",
    "\n",
    "# plot correlation matrix\n",
    "visualize.correlation_matrix(corr)\n",
    "\n",
    "df = drop_high_corr(df, corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_engineering import frequency_encoding, new_features\n",
    "\n",
    "df = new_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import scale_features\n",
    "\n",
    "df = scale_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take another peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.class_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import training\n",
    "\n",
    "X_resampled, y_resampled = training.resample(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 5.5 minutes to run\n",
    "\n",
    "# Initialize the model for RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "from src.model import create_model\n",
    "\n",
    "rfe_model = create_model(eval_metric='logloss')\n",
    "\n",
    "# Initialize RFE\n",
    "rfe = RFE(estimator=rfe_model, n_features_to_select=10)\n",
    "\n",
    "# Fit RFE\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data\n",
    "cols = X_train.columns[rfe.support_]\n",
    "X_train = pd.DataFrame(rfe.transform(X_train), columns=cols) # type: ignore\n",
    "X_test = pd.DataFrame(rfe.transform(X_test), columns=cols) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(eval_metric='logloss')\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "visualize.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "visualize.roc_curve(y_test, y_proba, roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f\"Cross-Validation ROC-AUC Scores: {cv_scores}\")\n",
    "print(f\"Mean Cross-Validation ROC-AUC Score: {cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlations between the features in X_train and the target variable y_train\n",
    "df_train = pd.concat([X_train, y_train])\n",
    "correlations = df_train.corr()['loan_status'].sort_values().to_frame()\n",
    "\n",
    "# plot the correlations\n",
    "visualize.correlations(correlations.drop('loan_status'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate permutation importance\n",
    "result = permutation_importance(model, X_train, y_train, n_repeats=3, random_state=42)\n",
    "\n",
    "# Get importance values\n",
    "importances = result['importances_mean']\n",
    "\n",
    "# Create a DataFrame with feature names and importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "feature_importances = feature_importances.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Display the feature importances DataFrame\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(feature_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "### HP search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    \"max_depth\": stats.randint(2, 3),\n",
    "    \"learning_rate\": stats.uniform(loc=0.93, scale=0.07),\n",
    "    \"n_estimators\": stats.randint(100, 1000),\n",
    "    \"subsample\": stats.norm(0.85, scale=0.05),\n",
    "    \"colsample_bytree\": stats.uniform(loc=0.98, scale=0.02),\n",
    "}\n",
    "\n",
    "# Setup the randomized search with 50 iterations\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=create_model(eval_metric='logloss'),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30, # ~9sec/iter\n",
    "    cv=3,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    n_jobs=(-1)\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Best Model\n",
    "print(\"Best Parameters:\", search.best_params_)\n",
    "print(f\"Best ROC-AUC Score: {search.best_score_:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain with these hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = search.best_params_\n",
    "model_best = create_model(**best_params)\n",
    "model_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred_best = model_best.predict(X_test)\n",
    "y_proba_best = model_best.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Accuracy\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Optimized Accuracy: {accuracy_best:.4f}\")\n",
    "\n",
    "print(\"Optimized Classification Report:\\n\", classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_proba_best)\n",
    "print(f\"Optimized ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "visualize.roc_curve(y_test, y_proba_best, roc_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
